{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPj6IWwJag3oO/m3ZfQ1LHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishpatidar123/ashishpatidar123/blob/main/Model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fulrEMqxVnso",
        "outputId": "664ed004-1095-4d0a-d0d9-df389bf343ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MyPMW8zUBIk",
        "outputId": "a062326c-68ae-4b67-f60c-31ba4d60c724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.4)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 6, 1, 8, 52  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv3d (Conv3D)                (None, 6, 1, 8, 12)  65532       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 6, 1, 8, 12)  48         ['conv3d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 6, 1, 8, 12)  0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling3d (MaxPooling3D)   (None, 6, 1, 8, 12)  0           ['re_lu[0][0]']                  \n",
            "                                                                                                  \n",
            " conv3d_1 (Conv3D)              (None, 6, 1, 8, 24)  30264       ['max_pooling3d[0][0]']          \n",
            "                                                                                                  \n",
            " conv3d_2 (Conv3D)              (None, 6, 1, 8, 48)  121008      ['conv3d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv3d_3 (Conv3D)              (None, 6, 1, 8, 12)  60492       ['conv3d_2[0][0]']               \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 6, 1, 8, 12)  0           ['max_pooling3d[0][0]',          \n",
            "                                                                  'conv3d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 6, 1, 8, 12)  48         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 6, 1, 8, 12)  0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv3d_4 (Conv3D)              (None, 6, 1, 8, 24)  30264       ['re_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv3d_5 (Conv3D)              (None, 6, 1, 8, 48)  121008      ['conv3d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv3d_6 (Conv3D)              (None, 6, 1, 8, 12)  60492       ['conv3d_5[0][0]']               \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 6, 1, 8, 12)  0           ['re_lu_1[0][0]',                \n",
            "                                                                  'conv3d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 6, 1, 8, 12)  48         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 6, 1, 8, 12)  0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 6, 1, 8, 12)  0           ['max_pooling3d[0][0]',          \n",
            "                                                                  're_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv3d_7 (Conv3D)              (None, 6, 1, 8, 2)   3530        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 6, 1, 8, 2)  8           ['conv3d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 6, 1, 8, 2)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling3d_1 (MaxPooling3D)  (None, 1, 1, 4, 2)  0           ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8)            0           ['max_pooling3d_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 832)          7488        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 2, 1, 8, 52)  0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 500,230\n",
            "Trainable params: 500,154\n",
            "Non-trainable params: 76\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#import all the required libraries\n",
        "import tensorflow as tf\n",
        "from keras.layers import Conv3D,BatchNormalization,ReLU,Flatten,Reshape,MaxPool3D, add ,Dense\n",
        "from keras.models import Model\n",
        "from math import floor\n",
        "import numpy as np\n",
        "\n",
        "#Now we store the dimensions of the input.\n",
        "\n",
        "Nr=1\n",
        "Nt=8\n",
        "K=52\n",
        "# define the network\n",
        "def network(x):\n",
        "\n",
        "    def ConvBlock1(y):\n",
        "        y = Conv3D(4*L, kernel_size = (3,7,5),padding = 'same', strides =(1,1,1))(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = ReLU()(y)\n",
        "        return y\n",
        "    \n",
        "    def Maxpool1(y):\n",
        "        y = MaxPool3D(pool_size = (3,3,3), padding = 'same', strides = (1,1,1))(y)\n",
        "        return y\n",
        "    \n",
        "    def ConvResBlock(y):\n",
        "        \n",
        "        temp = y\n",
        "        y = Conv3D(8*L, kernel_size = (3,7,5), padding = 'same', strides = (1,1,1))(y)\n",
        "        y = Conv3D(16*L, kernel_size = (3,7,5), padding = 'same', strides = (1,1,1))(y)\n",
        "        y = Conv3D(4*L, kernel_size = (3,7,5), padding = 'same', strides = (1,1,1))(y)\n",
        "        y = add([temp,y])\n",
        "\n",
        "        y = BatchNormalization()(y)\n",
        "        y = ReLU()(y)\n",
        "\n",
        "        return y\n",
        "    \n",
        "    def ConvBlock2(y):\n",
        "        y = Conv3D(2, kernel_size = (3,7,7),padding = 'same', strides =(1,1,1))(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = ReLU()(y)\n",
        "        return y\n",
        "    \n",
        "    def Maxpool2(y):\n",
        "        y = MaxPool3D(pool_size = (4,1,2), padding ='valid', strides = (4,1,2))(y)\n",
        "        return y\n",
        "    \n",
        "    def FCBlock(y):\n",
        "        #y = y.Reshape((B,2,math.floor(Nr/1),math.floor(Nr/2),math.floor(Nr/4)))(y)\n",
        "        y = Flatten()(x)\n",
        "        y = Dense(2*Nr*Nt*K, activation = 'linear')(y)\n",
        "        y = Reshape((2,Nr,Nt,K))(y)\n",
        "        return y\n",
        "    \n",
        "    \n",
        "    x = ConvBlock1(x)\n",
        "    x = Maxpool1(x)\n",
        "    temp2 = x\n",
        "    x = ConvResBlock(x)\n",
        "    x = ConvResBlock(x)\n",
        "    x = add([temp2,x])\n",
        "\n",
        "    x = ConvBlock2(x)\n",
        "    x = Maxpool2(x)\n",
        "    x = FCBlock(x)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  \n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.callbacks import TensorBoard, Callback\n",
        "import matplotlib.pyplot as py\n",
        "import numpy as np\n",
        "# import mat73\n",
        "import time\n",
        "import math\n",
        "\n",
        "# from NeuralNetwork import Neural_Network\n",
        "batch_sizer = 512\n",
        "Nr = 1\n",
        "Nt = 8\n",
        "K = 52\n",
        "\n",
        "!pip install h5py\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "L = 3\n",
        "\n",
        "\n",
        "# (52, 8, 1, 8, 85779)\n",
        "# (52, 8, 1, 8, 22222)\n",
        "\n",
        "# print(input_train_samples.shape)\n",
        "# print(input_test_samples.shape)\n",
        "# print(output_train_samples.shape)\n",
        "# print(output_test_samples.shape)\n",
        "\n",
        "# (85779, 6, 1, 8, 52)\n",
        "# (22222, 6, 1, 8, 52)\n",
        "# (85779, 2, 1, 8, 52)\n",
        "# (22222, 2, 1, 8, 52)\n",
        "# I'm getting ram error so I created the modified files in advance\n",
        "input_train_samples = np.load('/content/drive/MyDrive/Datasets/input_train_samples_modified.npy')\n",
        "output_train_samples = np.load('/content/drive/MyDrive/Datasets/output_train_samples_modified.npy')\n",
        "\n",
        "# defining the input tensor\n",
        "input_tensor = Input(shape=(2*L, Nr, Nt, K))\n",
        "output_value = network(input_tensor)\n",
        "\n",
        "initial_lr = 1e-3\n",
        "# defining the optimiser, loss function for out model\n",
        "CsiNet = Model(inputs=[input_tensor], outputs=[output_value])\n",
        "CsiNet.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "    learning_rate=initial_lr), loss='mse')\n",
        "print(CsiNet.summary())\n",
        "\n",
        "lr_drop_period1 = 100\n",
        "lr_drop_period2 = 200\n",
        "lr_drop_period3 = 250\n",
        "lr_drop_factor = 0.1\n",
        "\n",
        "# this piece of code will help in storing the losses\n",
        "class LossHistory(Callback):\n",
        "    # At the start of our training we intialize 2 variables to keep track of our losses\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses_train = []\n",
        "        self.losses_val = []\n",
        "    # We append to the end of the losses_train our current batch loss\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses_train.append(logs.get('loss'))\n",
        "    # We append to the end of the losses_val our validation loss\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses_val.append(logs.get('val_loss'))\n",
        "\n",
        "    def scheduler(epoch, lr):\n",
        "        if lr_drop_period1 == np.Inf or np.mod(epoch, lr_drop_period1) != 0 or lr_drop_period2 == np.Inf or np.mod(epoch, lr_drop_period2) != 0 or lr_drop_period3 == np.Inf or np.mod(epoch, lr_drop_period3) != 0:\n",
        "            return lr\n",
        "        else:\n",
        "            return lr * tf.math.exp(-lr_drop_factor)\n",
        "\n",
        "\n",
        "history = LossHistory()\n",
        "# Create a file name\n",
        "file = 'CsiNet_'+'_dim'+time.strftime('_%m_%d')\n",
        "path = 'result/TensorBoard_%s' % file\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=200, restore_best_weights=True)\n",
        "#  We now train our model with our data with number of steps=1000, (samples per size =200) and shuffle data with each step\n",
        "#  Also validate using x_val as both input and target\n",
        "#  We also need to record the losses using callbacks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CsiNet.fit(input_train_samples, output_train_samples,\n",
        "           epochs=300,\n",
        "           batch_size=512,\n",
        "           validation_data=(input_train_samples, output_train_samples),\n",
        "           shuffle=True,\n",
        "           callbacks=[history,\n",
        "                      TensorBoard(log_dir=path), earlyStopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq_YCV9Uwygm",
        "outputId": "9369fa7c-f5bb-4bbc-99f5-40bbe589c8df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "  6/168 [>.............................] - ETA: 11s - loss: 0.0171"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0330s vs `on_train_batch_end` time: 0.0445s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168/168 [==============================] - 34s 107ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 2/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 3/300\n",
            "168/168 [==============================] - 15s 90ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 4/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 5/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 6/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 7/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 8/300\n",
            "168/168 [==============================] - 14s 86ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 9/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 10/300\n",
            "168/168 [==============================] - 16s 98ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 11/300\n",
            "168/168 [==============================] - 15s 87ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 12/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 13/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 14/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 15/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 16/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 17/300\n",
            "168/168 [==============================] - 15s 87ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 18/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 19/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 20/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 21/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 22/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 23/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 24/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 25/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 26/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 27/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 28/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 29/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 30/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 31/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 32/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 33/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 34/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 35/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 36/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 37/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 38/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 39/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 40/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 41/300\n",
            "168/168 [==============================] - 16s 96ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 42/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 43/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 44/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 45/300\n",
            "168/168 [==============================] - 15s 91ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 46/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 47/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 48/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 49/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 50/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 51/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 52/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 53/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 54/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 55/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 56/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 57/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 58/300\n",
            "168/168 [==============================] - 15s 91ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 59/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 60/300\n",
            "168/168 [==============================] - 15s 91ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 61/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 62/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 63/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 64/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 65/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 66/300\n",
            "168/168 [==============================] - 15s 88ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 67/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 68/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 69/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 70/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 71/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 72/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 73/300\n",
            "168/168 [==============================] - 15s 91ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 74/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 75/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 76/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 77/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 78/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 79/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 80/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 81/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 82/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 83/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 84/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 85/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 86/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 87/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 88/300\n",
            "168/168 [==============================] - 14s 86ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 89/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 90/300\n",
            "168/168 [==============================] - 16s 97ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 91/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 92/300\n",
            "168/168 [==============================] - 15s 88ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 93/300\n",
            "168/168 [==============================] - 16s 97ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 94/300\n",
            "168/168 [==============================] - 15s 88ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 95/300\n",
            "168/168 [==============================] - 15s 88ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 96/300\n",
            "168/168 [==============================] - 15s 87ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 97/300\n",
            "168/168 [==============================] - 16s 96ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 98/300\n",
            "168/168 [==============================] - 16s 95ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 99/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 100/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 101/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 102/300\n",
            "168/168 [==============================] - 15s 91ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 103/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 104/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 105/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 106/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 107/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 108/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 109/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 110/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 111/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 112/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 113/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 114/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 115/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 116/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 117/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 118/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 119/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 120/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 121/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 122/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 123/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 124/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 125/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 126/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 127/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 128/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 129/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 130/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 131/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 132/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 133/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 134/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 135/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 136/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 137/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 138/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 139/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 140/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 141/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 142/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 143/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 144/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 145/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 146/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 147/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 148/300\n",
            "168/168 [==============================] - 15s 90ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 149/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 150/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 151/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 152/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 153/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 154/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 155/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 156/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 157/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 158/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 159/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 160/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 161/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 162/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 163/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 164/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 165/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 166/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 167/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 168/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 169/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 170/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 171/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 172/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 173/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 174/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 175/300\n",
            "168/168 [==============================] - 16s 95ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 176/300\n",
            "168/168 [==============================] - 16s 96ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 177/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 178/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 179/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 180/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 181/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 182/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 183/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 184/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 185/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 186/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 187/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 188/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 189/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 190/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 191/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 192/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 193/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 194/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 195/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 196/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 197/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 198/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 199/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 200/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 201/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 202/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 203/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 204/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 205/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 206/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 207/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 208/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 209/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 210/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 211/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 212/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 213/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 214/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 215/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 216/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 217/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 218/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 219/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 220/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 221/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 222/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 223/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 224/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 225/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 226/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 227/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 228/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 229/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 230/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 231/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 232/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 233/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 234/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 235/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 236/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 237/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 238/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 239/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 240/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 241/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 242/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 243/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 244/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 245/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 246/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 247/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 248/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 249/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 250/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 251/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 252/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 253/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 254/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 255/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 256/300\n",
            "168/168 [==============================] - 16s 96ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 257/300\n",
            "168/168 [==============================] - 16s 96ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 258/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 259/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 260/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 261/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 262/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 263/300\n",
            "168/168 [==============================] - 14s 84ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 264/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 265/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 266/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 267/300\n",
            "168/168 [==============================] - 16s 95ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 268/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 269/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 270/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 271/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 272/300\n",
            "168/168 [==============================] - 15s 89ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 273/300\n",
            "168/168 [==============================] - 16s 94ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 274/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 275/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 276/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 277/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 278/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 279/300\n",
            "168/168 [==============================] - 14s 85ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 280/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 281/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 282/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 283/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 284/300\n",
            "168/168 [==============================] - 16s 93ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 285/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 286/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 287/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 288/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 289/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 290/300\n",
            "168/168 [==============================] - 14s 82ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 291/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 292/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 293/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 294/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 295/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 296/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 297/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 298/300\n",
            "168/168 [==============================] - 14s 83ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 299/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 300/300\n",
            "168/168 [==============================] - 15s 92ms/step - loss: 0.0032 - val_loss: 0.0032\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4bead47550>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'result/trainloss_%s.csv' % file\n",
        "loss_history = np.array(history.losses_train)\n",
        "np.savetxt(filename, loss_history, delimiter=\",\")\n",
        "\n",
        "filename = 'result/valloss_%s.csv' % file\n",
        "loss_history = np.array(history.losses_val)\n",
        "np.savetxt(filename, loss_history, delimiter=\",\")"
      ],
      "metadata": {
        "id": "tW01Ew2ZF3zG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mat73\n",
        "\n",
        "import mat73 \n",
        "\n",
        "test_mat_file = mat73.loadmat('/content/drive/MyDrive/Datasets/test_prediction.mat')\n",
        "x_test = test_mat_file[\"HT\"]\n",
        "x_test = x_test.astype(float)\n",
        "input_test_samples = np.concatenate((x_test[:, 0:3], x_test[:, 4:7]), axis=1)\n",
        "output_test_samples = np.concatenate((x_test[:, 3:4], x_test[:, 7:8]), axis=1)\n",
        "input_test_samples = np.reshape(input_test_samples, (-1, 2*L, Nr, Nt, K))\n",
        "output_test_samples = np.reshape(output_test_samples, (-1, 2, Nr, Nt, K))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-7U1bftF7Re",
        "outputId": "29eb4e0d-27b0-4941-d463-d08cb84e326b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mat73\n",
            "  Downloading mat73-0.60-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mat73) (1.22.4)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now is the time to test our model and store in x_hat\n",
        "tStart = time.time()\n",
        "x_hat = CsiNet.predict(input_test_samples)\n",
        "tEnd = time.time()\n",
        "print(\"It cost %f sec\" % ((tEnd - tStart)/input_test_samples.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZMKGlfoF6_Q",
        "outputId": "873759e9-d807-4cfe-cf5c-cb6749c8d709"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "695/695 [==============================] - 4s 5ms/step\n",
            "It cost 0.000308 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def NMSE(Hi, Hi_hat):\n",
        "    Hi_real = np.reshape(Hi[:, 0, :, :, :], (len(Hi), -1))\n",
        "    Hi_img = np.reshape(Hi[:, 1, :, :, :], (len(Hi), -1))\n",
        "    Hi_complex = (Hi_real-0.5)+1j*(Hi_img-0.5)\n",
        "\n",
        "    Hi_hat_real = np.reshape(Hi_hat[:, 0, :, :, :], (len(Hi_hat), -1))\n",
        "    Hi_hat_img = np.reshape(Hi_hat[:, 1, :, :, :], (len(Hi_hat), -1))\n",
        "    Hi_hat_complex = (Hi_hat_real-0.5)+1j*(Hi_hat_img-0.5)\n",
        "\n",
        "    mse = np.sum(np.square(np.abs(Hi_complex - Hi_hat_complex)), axis=(1))\n",
        "    power = np.sum(np.abs(Hi_complex) ** 2, axis=(1))\n",
        "    NMSE = 10 * np.log10(np.mean(mse) / np.maximum(np.mean(power), 1e-10))\n",
        "    return NMSE\n",
        "\n",
        "\n",
        "\n",
        "# print(x_hat.shape)\n",
        "# print(output_test_samples.shape)\n",
        "\n",
        "print(\"NMSE is \", NMSE(output_test_samples, x_hat))\n",
        "filename = \"result/decoded_%s.csv\" % file\n",
        "x_hat1 = np.reshape(x_hat, (len(x_hat), -1))\n",
        "np.savetxt(filename, x_hat1, delimiter=\",\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwGW4rL1p624",
        "outputId": "d0acd2ee-d8be-431f-de4e-75f29fdb31c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMSE is  -13.961168386732453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OipAYHi9IgGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize CSINET model to JSON\n",
        "model_json = CsiNet.to_json()\n",
        "outfile = \"result/model_%s.json\" % file\n",
        "with open(outfile, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize weights to HDF5\n",
        "outfile = \"result/model_%s.h5\" % file\n",
        "CsiNet.save_weights(outfile)\n",
        "\n",
        "# Save model in tensorflow format\n",
        "CsiNet.save(\"tensorflow/model_%s\" % file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hMOXqJVGJ3I",
        "outputId": "5ab55771-35a9-434e-a4f4-af3e0a8f0656"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first we do some data preprocessing that is extracting what is required from\n",
        "# the given data and reshape it into the required input shape\n",
        "# same for the output data\n",
        "# and same for the test data"
      ],
      "metadata": {
        "id": "fDF7GEwV5Tq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in this piece of code we're trying to fit the model\n",
        "# we're keeping number of epochs as 300 and a batch size of 512"
      ],
      "metadata": {
        "id": "RdWi_Oqo5TnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we will find the normalised mean squared error\n",
        "# and compare it with the actual results we got in the research paper"
      ],
      "metadata": {
        "id": "YntRb1vi5TbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/result', 'zip', '/content/result')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pMfiTdMq5TNS",
        "outputId": "7799cf88-79e7-46d9-f6bf-3f71e9bdd138"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/result.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/result.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "P8spwxOz4GhG",
        "outputId": "8cd25278-f82e-4e15-b874-0da43368a28b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5f4b484d-d83b-48b5-ab1b-f958367bae81\", \"result.zip\", 199889359)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/tensorflow', 'zip', '/content/tensorflow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CAxULFlQM_Dm",
        "outputId": "4ead7f68-cff0-4afa-d36d-281c0be1ca68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tensorflow.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/tensorflow.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ondUIeZWM-2b",
        "outputId": "bbf410c2-6c0e-4767-855e-5b58ff138ed4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_486bbab6-e759-4f9f-a781-8c6df83bb22e\", \"tensorflow.zip\", 2481695)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "La_DXEOGRGiv"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}
